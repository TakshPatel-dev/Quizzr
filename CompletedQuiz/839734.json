{"0":{"id":2282,"question":"What is the most efficient way to copy large amounts of data between PostgreSQL tables?","description":"Understanding bulk data operations in PostgreSQL.","answers":{"answer_a":"Using INSERT INTO SELECT","answer_b":"Using multiple INSERT statements","answer_c":"Using a cursor to fetch and insert","answer_d":"Exporting to CSV and importing back","answer_e":null,"answer_f":null},"multiple_correct_answers":"false","correct_answers":{"answer_a_correct":"true","answer_b_correct":"false","answer_c_correct":"false","answer_d_correct":"false","answer_e_correct":"false","answer_f_correct":"false"},"correct_answer":null,"explanation":"INSERT INTO SELECT is the most efficient way to copy large amounts of data between tables as it's executed entirely within the database engine, avoiding the overhead of multiple statements or external file operations.","tip":null,"tags":[{"name":"Postgres"}],"category":"Postgres","difficulty":"Medium"},"1":{"id":2372,"question":"Which configuration best improves sort performance for large datasets?","description":"Understanding sort optimization.","answers":{"answer_a":"Increase shared_buffers","answer_b":"Increase work_mem and maintenance_work_mem","answer_c":"Enable parallel_tuple_cost","answer_d":"Adjust effective_cache_size","answer_e":null,"answer_f":null},"multiple_correct_answers":"false","correct_answers":{"answer_a_correct":"false","answer_b_correct":"true","answer_c_correct":"false","answer_d_correct":"false","answer_e_correct":"false","answer_f_correct":"false"},"correct_answer":null,"explanation":"Increasing work_mem and maintenance_work_mem allows more sorting to be done in memory, reducing disk I/O and improving sort performance for large datasets.","tip":null,"tags":[{"name":"Postgres"}],"category":"Postgres","difficulty":"Medium"},"2":{"id":2364,"question":"What is the most effective way to optimize sequential scans on large tables?","description":"Understanding sequential scan optimization.","answers":{"answer_a":"Increase work_mem","answer_b":"Adjust effective_io_concurrency and maintenance_io_concurrency","answer_c":"Add more indexes","answer_d":"Use VACUUM FULL","answer_e":null,"answer_f":null},"multiple_correct_answers":"false","correct_answers":{"answer_a_correct":"false","answer_b_correct":"true","answer_c_correct":"false","answer_d_correct":"false","answer_e_correct":"false","answer_f_correct":"false"},"correct_answer":null,"explanation":"Adjusting effective_io_concurrency and maintenance_io_concurrency optimizes parallel I/O operations during sequential scans, improving read performance on systems with good I/O capability.","tip":null,"tags":[{"name":"Postgres"}],"category":"Postgres","difficulty":"Medium"},"3":{"id":2335,"question":"What is the best practice for handling large result sets in PostgreSQL client applications?","description":"Understanding result set handling in PostgreSQL.","answers":{"answer_a":"Fetch all results at once","answer_b":"Use cursor-based fetching with DECLARE CURSOR","answer_c":"Limit results arbitrarily","answer_d":"Use temporary tables","answer_e":null,"answer_f":null},"multiple_correct_answers":"false","correct_answers":{"answer_a_correct":"false","answer_b_correct":"true","answer_c_correct":"false","answer_d_correct":"false","answer_e_correct":"false","answer_f_correct":"false"},"correct_answer":null,"explanation":"Cursor-based fetching allows efficient handling of large result sets by retrieving data in manageable chunks, preventing memory exhaustion in both server and client.","tip":null,"tags":[{"name":"Postgres"}],"category":"Postgres","difficulty":"Medium"},"4":{"id":2288,"question":"What is the best practice for handling enum-like data in PostgreSQL?","description":"Understanding data type choices in PostgreSQL.","answers":{"answer_a":"Using VARCHAR with CHECK constraints","answer_b":"Using ENUM type","answer_c":"Using lookup tables with foreign keys","answer_d":"Using INTEGER with CHECK constraints","answer_e":null,"answer_f":null},"multiple_correct_answers":"false","correct_answers":{"answer_a_correct":"false","answer_b_correct":"false","answer_c_correct":"true","answer_d_correct":"false","answer_e_correct":"false","answer_f_correct":"false"},"correct_answer":null,"explanation":"While PostgreSQL supports ENUM types, using lookup tables with foreign keys is more flexible as it allows easy addition of new values, supports additional attributes, and maintains referential integrity while providing good performance.","tip":null,"tags":[{"name":"Postgres"}],"category":"Postgres","difficulty":"Medium"},"5":{"id":2267,"question":"Which PostgreSQL feature would you use to automatically update a 'last_modified' timestamp column whenever a row is updated?","description":"Understanding PostgreSQL's automatic column updates.","answers":{"answer_a":"AUTO_UPDATE constraint","answer_b":"TRIGGER function","answer_c":"GENERATED ALWAYS AS expression","answer_d":"DEFAULT now() with BEFORE UPDATE trigger","answer_e":null,"answer_f":null},"multiple_correct_answers":"false","correct_answers":{"answer_a_correct":"false","answer_b_correct":"true","answer_c_correct":"false","answer_d_correct":"false","answer_e_correct":"false","answer_f_correct":"false"},"correct_answer":null,"explanation":"While all options might work, a TRIGGER function is the most flexible and standard way to automatically update a timestamp column on row updates. It can be implemented using a simple BEFORE UPDATE trigger that sets last_modified = now().","tip":null,"tags":[{"name":"Postgres"}],"category":"Postgres","difficulty":"Medium"},"6":{"id":2401,"question":"What is the most efficient way to handle frequent updates to statistics tables?","description":"Understanding statistical data optimization.","answers":{"answer_a":"Regular table with indexes","answer_b":"Unlogged table with periodic snapshots","answer_c":"Temporary tables","answer_d":"Memory tables","answer_e":null,"answer_f":null},"multiple_correct_answers":"false","correct_answers":{"answer_a_correct":"false","answer_b_correct":"true","answer_c_correct":"false","answer_d_correct":"false","answer_e_correct":"false","answer_f_correct":"false"},"correct_answer":null,"explanation":"Unlogged tables with periodic snapshots provide efficient handling of frequent statistical updates by avoiding WAL overhead while maintaining data persistence through snapshots.","tip":null,"tags":[{"name":"Postgres"}],"category":"Postgres","difficulty":"Medium"},"7":{"id":2268,"question":"In PostgreSQL, what is the most efficient way to prevent duplicate rows based on multiple columns?","description":"Understanding constraint optimization in PostgreSQL.","answers":{"answer_a":"Check for duplicates in application code","answer_b":"Create a UNIQUE constraint on the combined columns","answer_c":"Use a trigger to check for duplicates","answer_d":"Create separate UNIQUE constraints for each column","answer_e":null,"answer_f":null},"multiple_correct_answers":"false","correct_answers":{"answer_a_correct":"false","answer_b_correct":"true","answer_c_correct":"false","answer_d_correct":"false","answer_e_correct":"false","answer_f_correct":"false"},"correct_answer":null,"explanation":"Creating a UNIQUE constraint on multiple columns is the most efficient way to prevent duplicates as it creates an index that PostgreSQL can use for both enforcing uniqueness and improving query performance.","tip":null,"tags":[{"name":"Postgres"}],"category":"Postgres","difficulty":"Medium"},"8":{"id":2376,"question":"Which approach best optimizes performance for tables with frequent updates to indexed columns?","description":"Understanding index maintenance optimization.","answers":{"answer_a":"Remove indexes","answer_b":"Use HOT updates with appropriate fillfactor","answer_c":"Rebuild indexes regularly","answer_d":"Use expression indexes","answer_e":null,"answer_f":null},"multiple_correct_answers":"false","correct_answers":{"answer_a_correct":"false","answer_b_correct":"true","answer_c_correct":"false","answer_d_correct":"false","answer_e_correct":"false","answer_f_correct":"false"},"correct_answer":null,"explanation":"HOT (Heap-Only Tuple) updates with appropriate fillfactor reduce index maintenance overhead by allowing updates to bypass index updates when possible.","tip":null,"tags":[{"name":"Postgres"}],"category":"Postgres","difficulty":"Medium"},"9":{"id":2273,"question":"What is the most appropriate PostgreSQL data type for storing IPv4 and IPv6 addresses?","description":"Understanding PostgreSQL's specialized data types.","answers":{"answer_a":"VARCHAR","answer_b":"TEXT","answer_c":"INET","answer_d":"BYTEA","answer_e":null,"answer_f":null},"multiple_correct_answers":"false","correct_answers":{"answer_a_correct":"false","answer_b_correct":"false","answer_c_correct":"true","answer_d_correct":"false","answer_e_correct":"false","answer_f_correct":"false"},"correct_answer":null,"explanation":"The INET data type is specifically designed for storing IPv4 and IPv6 addresses. It provides input validation, specialized indexing and operators, and more efficient storage compared to text-based alternatives.","tip":null,"tags":[{"name":"Postgres"}],"category":"Postgres","difficulty":"Medium"},"quizMetaData":{"createdBy":"99692866465","difficulty":"medium","topic":"postgres","totalQuestions":"10","createdAt":"25/05/2025\t01:08:19","description":"sdfsdfd","time":"30"},"marks":"0","answer":{}}